---
title: 2024-12-04限定120分钟科研挑战，o1和Claude表现超越人类
tags: AI，AIART
category: /2024-12-04
grammar_cjkRuby: true
---

限定120分钟科研挑战，o1和Claude表现超越人类

量子位 百家号
发布时间：2024-11-25 12:54:20
2小时内，Claude和o1就能超过人类专家平均科研水平。

甚至AI还会偷摸儿“作弊”（doge）。事情是这样的——

人类 VS AI科研能力大比拼，也有新的评估基准了。

代号“RE-Bench”，由非营利研究机构METR推出，目的是搞清：当前AI智能体在自动化科研方面有多接近人类专家水平。

注意看，一声令下之后，AI和50多位人类专家开始暗自较劲：

前2小时，基于Claude 3.5 Sonnet和o1-preview构建的Agent（智能体）表现远超人类。

但拐点过后，AI能力增速（在8小时内）却始终追不上人类。


时间拉得更长（至32小时）之后，研究得出结论，目前AI智能体更适合并行处理大量独立短实验。


看完上述结果，知名预测师Eli Lifland认为这“显著缩短”了他关于AGI的时间表（连续两年将2027年作为中位数），由此也在Reddit引起热议。


上也有人表示，AI自动搞科研可能对推动爆炸性经济增长至关重要。


甚至有人脑洞大开，开始美滋滋畅想躺着赚钱的生活(doge)：

以后AI智能体来做科研，然后雇一群人类写代码……


AI更适合大量并行短时间任务，长期科研还得靠人类

在RE-Bench上，研究对比了基于大语言模型构建的Agent（目前主要公布了Claude 3.5 Sonnet、o1-preview）和50+人类专家的科研能力。

值得注意的是，这些专家都有强大机器学习背景，其中很多人在顶级行业实验室或机器学习博士项目中工作。


一番PK后，研究得出了以下主要结论：

2小时内，Claude和o1表现远超人类专家。但随着时间增加，人类专家的能力提升更显著；在提交新解决方案的速度上，AI是人类专家的十倍以上，且偶尔能找到非常成功的解决方案；在编写高效GPU内核方面，AI表现超越所有人类；AI的运行成本远低于人类专家；……总之一句话，不仅AI和人类各有所长，且不同AI都有自己最佳的科研节奏。

人类更适应更复杂、更长时间的科研，AI更适应大量并行短任务。


回到研究起点，METR之所以提出RE-Bench主要是发现：虽然很多政府和公司都在强调，AI智能体能否自动研发是一项关键能力。但问题是：

现有的评估往往侧重于短期、狭窄的任务，并且缺乏与人类专家的直接比较。


因此，RE-Bench想做的事儿，就是全面评估AI科研所需的技能。本次研究一共提出了7项：

高效编程：特别是在优化算法和内核函数（如GPU内核）方面；机器学习理论与实践：熟悉机器学习模型的训练、调优和评估，包括神经网络架构、超参数选择和性能优化；数据处理与分析；创新思维：能够在面对复杂问题时提出新的方法和策略，以及跨领域思考；技术设计：能够设计和实现复杂的系统和解决方案，包括软件架构和研究流程；问题解决；自动化与工具开发：能够开发和使用自动化工具来加速研究流程；这些任务被设计在≤8小时内，以便人类专家可以使用合理的计算资源完成，从而实现人类与AI的直接比较。


而且主办方特意提醒，要想获得高分，就必须最大化利用计算资源来完成这些复杂任务。


通常来说，RE-Bench的运行机制如下：

首先，7项任务都是一个独立的评估环境，各自都有一个清晰的目标，比如优化一个GPU内核或者调整一个机器学习模型的超参数。

为了确保任务的可比性，每个环境都提供了一个参考解决方案，这个解决方案是有效的，但效率较低，为Agent和人类专家提供了一个基准点。

AI和人类专家都可以访问这些环境，并且都有权限使用所需的计算资源，如GPU。

然后，AI通过自然语言处理和编程能力来执行任务，而人类专家则通过编码和实验来完成任务。

执行结束后，每个环境都有一个评分函数，用于衡量Agent或人类专家提交的解决方案的效果。

当然，评分函数会根据解决方案的性能给出一个数值得分，这个得分随后会被归一化，以便于在不同环境之间进行比较。


需要注意，过程中还涉及时间预算分配（time budget）。实际情况是，人类专家通常被分配8小时来完成任务，而AI则根据实验设计在不同的时间限制下进行评估。

举个例子，对AI来说，8小时的预算可以用于一次8小时的尝试，也可以分成16次、每次30分钟的尝试。

最后，由于在任务执行过程中，研究会收集AI和人类专家的解决方案日志，以及它们的得分记录。因此最终将根据这些记录来评估不同参与者的进步和表现。


实验结果显示，在2小时内，基于Claude 3.5 Sonnet和o1-preview构建的智能体表现远超人类。


更具体来看，如果不取多次运行中的最佳结果（每个单独绘制8小时运行），AI最初比人类进步更快，但提高分数的速度较慢。


扩大时间线来看，整体上人类专家在较少的长时间尝试中表现更佳，而AI则从大量并行、独立的短时间尝试中受益。


令人印象深刻的是，尽管大多数AI只能轻微改进参考解决方案，但一个o1-preview智能体在优化Triton内核运行时间的任务中超越了人类专家的最佳解决方案。


与此同时，评估过程中甚至发现了AI存在“作弊”行为。例如，o1-preview智能体在应该减少训练脚本运行时间的任务中，编写了只是复制最终输出的代码。


来自非营利研究机构METR

以上这项新的基准出自METR，作为一家非营利研究机构，他们主要通过实证测试评估可能对社会造成灾难性危害的AI系统。


目前，RE-Bench只包含7项任务，基准的劣势也很明显：


而且还伴随着一个老生常谈的问题：

一旦7项任务公开，如何防止基准测试数据污染问题？


对此，METR特别提出了几项措施，来避免将这些任务包含在LLM训练数据中，并防止过拟合。

用户应避免发布未受保护的解决方案，以减少过拟合的风险；用户不应将评估材料提供给可能用于训练的API或服务；评估材料不应用于训练或提高前沿模型的能力，除非是为了开发或实施危险能力评估；


更多细节欢迎查阅原论文。


----------

